{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1087928-1d40-4bd4-a635-2cd6eb70d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install streamlit\n",
    "# !pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45530cf9-6bc3-467b-ad4b-9fca64c9aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://python.langchain.com/v0.1/docs/modules/memory/\n",
    "import time\n",
    "# from numba import cuda \n",
    "import gc\n",
    "# import torch\n",
    "\n",
    "from langchain_community.llms import LlamaCpp\n",
    "# from langchain.callbacks.manager import CallbackManager\n",
    "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69be735a-41d3-4a7e-9c37-8de8010495f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ..\\models\\llama-2-7b-chat.Q8_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elapsed time: 2.97 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '7', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "# first initialize the large language model\n",
    "model_path = '..\\models\\llama-2-7b-chat.Q8_0.gguf'  # https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "\n",
    "temperature = 0\n",
    "top_p = 1\n",
    "max_new_tokens = 500\n",
    "\n",
    "# Define model parameters\n",
    "time_1 = time.time()\n",
    "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "            model_path = model_path,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_new_tokens,\n",
    "            top_p = top_p,\n",
    "            n_gpu_layers = -1,\n",
    "            n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "            n_ctx = 4096,\n",
    "            f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            # callback_manager = callback_manager,\n",
    "            verbose = True, # Verbose is required to pass to the callback manager\n",
    "        )\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22286c48-5ca4-46f2-beca-95ea5f473bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elapsed time: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful, respectful and honest assistant. \n",
    "    Always answer as helpfully as possible.  \n",
    "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \n",
    "    If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            system_prompt\n",
    "        ),\n",
    "        # The `variable_name` here is what must align with memory\n",
    "        MessagesPlaceholder(variable_name = \"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
    "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name.\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages= True)\n",
    "conversation = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = prompt,\n",
    "    verbose = True,\n",
    "    memory = memory\n",
    ")\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaae02aa-093b-4a9d-bfde-bbab53652048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\014206631\\AppData\\Local\\miniconda3\\envs\\llamaChat\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful, respectful and honest assistant. \n",
      "    Always answer as helpfully as possible.  \n",
      "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \n",
      "    If you don't know the answer to a question, please don't share false information.\n",
      "\n",
      "Human: \n",
      "Tell me a slightly funny story.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4071.52 ms\n",
      "llama_print_timings:      sample time =      41.17 ms /   127 runs   (    0.32 ms per token,  3084.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4071.29 ms /    95 tokens (   42.86 ms per token,    23.33 tokens per second)\n",
      "llama_print_timings:        eval time =   31035.99 ms /   126 runs   (  246.32 ms per token,     4.06 tokens per second)\n",
      "llama_print_timings:       total time =   35563.31 ms /   221 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Assistant:  Of course! Here is a slightly funny story for you:\n",
      "Once upon a time, there was a chicken named Cluck who loved to play hide and seek with his friends. One day, while hiding behind a tree, he forgot to cover his beak and it got stuck in the branch. His friends found him and burst out laughing at the sight of him stuck there. Cluck was not amused and said, \"What's so funny? I'm just trying to hide!\"\n",
      "I hope you enjoyed that story! Is there anything else I can help you with?\n",
      "\n",
      "Elapsed time: 35.80 seconds\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Tell me a slightly funny story.\n",
    "\"\"\"\n",
    "answer_0 = conversation({\"question\": user_prompt})\n",
    "print(answer_0['text'])\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a573d808-79fc-4da5-a912-781a383f7cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful, respectful and honest assistant. \n",
      "    Always answer as helpfully as possible.  \n",
      "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \n",
      "    If you don't know the answer to a question, please don't share false information.\n",
      "\n",
      "Human: \n",
      "Tell me a slightly funny story.\n",
      "\n",
      "AI: Assistant:  Of course! Here is a slightly funny story for you:\n",
      "Once upon a time, there was a chicken named Cluck who loved to play hide and seek with his friends. One day, while hiding behind a tree, he forgot to cover his beak and it got stuck in the branch. His friends found him and burst out laughing at the sight of him stuck there. Cluck was not amused and said, \"What's so funny? I'm just trying to hide!\"\n",
      "I hope you enjoyed that story! Is there anything else I can help you with?\n",
      "Human: What is the name of the chicken?\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4071.52 ms\n",
      "llama_print_timings:      sample time =      18.53 ms /    57 runs   (    0.33 ms per token,  3076.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6456.20 ms /   142 tokens (   45.47 ms per token,    21.99 tokens per second)\n",
      "llama_print_timings:        eval time =   12992.08 ms /    56 runs   (  232.00 ms per token,     4.31 tokens per second)\n",
      "llama_print_timings:       total time =   19643.52 ms /   198 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " \n",
      "AI: Assistant: Oh, my apologies! The name of the chicken in the story is Cluck. I'm glad you liked it! If you have any other questions or would like to hear another slightly funny story, please let me know.\n",
      "\n",
      "Elapsed time: 19.65 seconds\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "\n",
    "user_prompt = \"What is the name of the chicken?\"\n",
    "answer_1 = conversation({\"question\": user_prompt})\n",
    "print(answer_1['text'])\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1edd198a-7ca0-4d19-8660-71bf1b8ddd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the name of the chicken?',\n",
       " 'chat_history': [HumanMessage(content='\\nTell me a slightly funny story.\\n'),\n",
       "  AIMessage(content='Assistant:  Of course! Here is a slightly funny story for you:\\nOnce upon a time, there was a chicken named Cluck who loved to play hide and seek with his friends. One day, while hiding behind a tree, he forgot to cover his beak and it got stuck in the branch. His friends found him and burst out laughing at the sight of him stuck there. Cluck was not amused and said, \"What\\'s so funny? I\\'m just trying to hide!\"\\nI hope you enjoyed that story! Is there anything else I can help you with?'),\n",
       "  HumanMessage(content='What is the name of the chicken?'),\n",
       "  AIMessage(content=' \\nAI: Assistant: Oh, good catch! The name of the chicken in the story is actually \"Cluck Norris.\" Cluck is a common name for chickens, and Norris is a popular last name. I apologize for the confusion earlier. Is there anything else you would like to know?')],\n",
       " 'text': ' \\nAI: Assistant: Oh, good catch! The name of the chicken in the story is actually \"Cluck Norris.\" Cluck is a common name for chickens, and Norris is a popular last name. I apologize for the confusion earlier. Is there anything else you would like to know?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdfca6a0-e542-48ed-8ba8-aa1614b8123c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "AI: Assistant: Oh, my apologies! The name of the chicken in the story is Cluck. I'm glad you liked it! If you have any other questions or would like to hear another slightly funny story, please let me know.\n"
     ]
    }
   ],
   "source": [
    "print(answer_1['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82e10da7-4514-42a0-9620-04e5461c26f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AI: Assistant: Oh, my apologies! The name of the chicken in the story is Cluck. I'm glad you liked it! If you have any other questions or would like to hear another slightly funny story, please let me know.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = answer_1['text'].strip()\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "239717a8-9b7d-46c7-8147-16be2ec02bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='\\nTell me a slightly funny story.\\n')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_1['chat_history'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a509155f-9bb3-4170-8f12-456313b4259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='\\nTell me a slightly funny story.\\n'\n"
     ]
    }
   ],
   "source": [
    "print(answer_1['chat_history'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b7acd498-c52c-4178-bf9a-7694dd83e252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='\\nTell me a slightly funny story.\\n')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_1['chat_history'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7c6d99f-836b-4271-964b-ea95abf1ec3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nAI: Assistant: Oh, my apologies! The name of the chicken in the story is Cluck. I'm glad you liked it! If you have any other questions or would like to hear another slightly funny story, please let me know.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cb51caa-0105-43f6-8d14-24f8890e908d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" \\\\nAI: Assistant: Oh, my apologies! The name of the chicken in the story is Cluck. I\\'m glad you liked it! If you have any other questions or would like to hear another slightly funny story, please let me know.\"'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b12d9eb5-486c-46af-86ae-f320ea94cc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "AI: Assistant: Oh, my apologies! The name of the chicken in the story is Cluck. I'm glad you liked it! If you have any other questions or would like to hear another slightly funny story, please let me know.\n"
     ]
    }
   ],
   "source": [
    "aa = print(answer_1['text'], flush = False, file = aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e334cf0-38d6-4b86-a0fe-e463158295bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "AI: Assistant: Oh, my apologies! The name of the chicken in the story is Cluck. I'm glad you liked it! If you have any other questions or would like to hear another slightly funny story, please let me know.\n",
      "\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "\n",
    "old_stdout = sys.stdout # Memorize the default stdout stream\n",
    "sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "print(answer_1['text'])\n",
    "\n",
    "sys.stdout = old_stdout # Put the old stream back in place\n",
    "\n",
    "aaa = buffer.getvalue() # Return a str containing the entire contents of the buffer.\n",
    "print(aaa) # Why not to print it?\n",
    "print(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5efe62f4-df5d-4584-b291-6a0a2f564182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AI: Assistant: Oh, my apologies! The name of the chicken in the story is Cluck. I'm glad you liked it! If you have any other questions or would like to hear another slightly funny story, please let me know.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d44d961-264c-4bca-b82b-5bab30337290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elapsed time: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "\n",
    "system_prompt = \"\"\"You are a serious assistant that don't tell jokes.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            system_prompt\n",
    "        ),\n",
    "        # The `variable_name` here is what must align with memory\n",
    "        MessagesPlaceholder(variable_name = \"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
    "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name.\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages= True)\n",
    "conversation = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = prompt,\n",
    "    verbose = True,\n",
    "    memory = memory\n",
    ")\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64f31470-5ad0-4bb0-8378-fafd297ffd65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a serious assistant that don't tell jokes.\n",
      "\n",
      "Human: \n",
      "Tell me a slightly funny story.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "System: I apologize, but I cannot fulfill that request as it goes against my programming rules rules rules rules to engage in humor or tell jokes. My purpose is to assist and provide helpful information, and I will continue to do so in a professional and respectful manner. Is there anything else I can help you with?\n",
      "\n",
      "Elapsed time: 40.02 seconds\n"
     ]
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Tell me a slightly funny story.\n",
    "\"\"\"\n",
    "answer_0 = conversation({\"question\": user_prompt})\n",
    "print(answer_0['text'])\n",
    "\n",
    "print()\n",
    "time_2 = time.time()\n",
    "time_1_2 = time_2 - time_1\n",
    "print(f'Elapsed time: {time_1_2:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7bbc31-5155-4d6f-9877-d35e1b09193d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d01731-5a47-439f-b86f-359064564cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2100944-a665-412b-90ac-57e562176626",
   "metadata": {},
   "source": [
    "## streamlit version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7a34b-0563-436b-9cf9-7cf2cb3d94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/shashankdeshpande/langchain-chatbot/blob/master/pages/2_%E2%AD%90_context_aware_chatbot.py\n",
    "https://langchain-chatbot.streamlit.app/context_aware_chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1453bb0-b00a-4266-932b-9fe141104df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db92aecc-45d1-4236-9328-db1c2deed48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st \n",
    "# From: https://python.langchain.com/v0.1/docs/modules/memory/\n",
    "import time\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "class llama2Chat:\n",
    "    def __init__(self):\n",
    "        # first initialize the large language model\n",
    "        self.model_path = '..\\models\\llama-2-7b-chat.Q8_0.gguf'  # https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "        self.temperature = 0\n",
    "        self.top_p = 1\n",
    "        self.max_new_tokens = 500\n",
    "        self.system_prompt = \"\"\"You are a helpful, respectful and honest assistant. \n",
    "    Always answer as helpfully as possible.  \n",
    "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \n",
    "    If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\"\n",
    "    \n",
    "    @st.cache_resource\n",
    "    def setup_chain(_self):\n",
    "        # Define model parameters\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        llm = LlamaCpp(\n",
    "                    model_path = _self.model_path,\n",
    "                    temperature = _self.temperature,\n",
    "                    max_tokens = _self.max_new_tokens,\n",
    "                    top_p = _self.top_p,\n",
    "                    n_gpu_layers = -1,\n",
    "                    n_batch = 512, # Should be between 1 and n_ctx, consider the amount of RAM\n",
    "                    n_ctx = 4096,\n",
    "                    f16_kv = True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "                    callback_manager = callback_manager,\n",
    "                    verbose = True, # Verbose is required to pass to the callback manager\n",
    "                )\n",
    "        \n",
    "        prompt = ChatPromptTemplate(\n",
    "            messages=[\n",
    "                SystemMessagePromptTemplate.from_template(\n",
    "                    _self.system_prompt\n",
    "                ),\n",
    "                # The `variable_name` here is what must align with memory\n",
    "                MessagesPlaceholder(variable_name = \"chat_history\"),\n",
    "                HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "            ]\n",
    "        )\n",
    "        memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages= True)\n",
    "        _self.conversation = LLMChain(\n",
    "            llm = llm,\n",
    "            prompt = prompt,\n",
    "            verbose = False,\n",
    "            memory = memory\n",
    "        )\n",
    "\n",
    "    def answer_prompt(_self, user_prompt):\n",
    "        _self.answer = _self.conversation({\"question\": user_prompt})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb2ef5-7c44-4eb0-bf61-8edc1eae7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "llamaChatObj = llama2Chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f8c8a-2074-42db-94d1-07361b0128d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llamaChatObj.setup_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b4aa2-30f0-4721-bdce-c1ce6adf3387",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "Tell me a slightly funny story.\n",
    "\"\"\"\n",
    "llamaChatObj.answer_prompt(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd481d41-afad-41c1-85c8-1ee8c45b4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llamaChatObj.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d9074-2c07-4f46-adc9-599583742da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c0900e-f9b3-4117-afad-d9cb25975095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
